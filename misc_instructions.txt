python -m torch.distributed.run --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=127.0.0.1 --master_port=29501 SADM-ddp.py
python SADM.py

for training the model run this command (~4 GPU):
torchrun --nproc_per_node=4 --nnodes=1 --node_rank=0 --master_addr="127.0.0.1" --master_port=29500 train_multigpu.py

for running the restart.sh script run this:
chmod +x restart.sh
./restart.sh

dataset creation:


import os, torch, numpy
subs=[]; files=[]
path = './dataset/'
for sp in ['ad_split', 'cn_split', 'mci_split']:
    sp_dir = os.path.join(path, sp, 'train')
    for f in os.listdir(sp_dir):
        subs.append(torch.load(os.path.join(sp_dir, f)))
        files.append(f)
subs = torch.stack(subs, dim=0).numpy()
numpy.save('./train_dat.npy', subs)
numpy.save('./train_files.npy', files)


import os, torch, numpy
subs=[]; files=[]
path = './dataset/'
for sp in ['ad_split', 'cn_split', 'mci_split']:
    sp_dir = os.path.join(path, sp, 'test')
    for f in os.listdir(sp_dir):
        subs.append(torch.load(os.path.join(sp_dir, f)))
        files.append(f)
subs = torch.stack(subs, dim=0).numpy()
numpy.save('./test_dat.npy', subs)
numpy.save('./test_files.npy', files)



save all 4 files under '/data/' inside SADM main directory.